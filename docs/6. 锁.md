# 6. 锁

大多数内核，包括xv6，将多个活动的执行交错在一起。多处理器硬件是交错的一个来源：独立执行的多个CPU的计算机，例如xv6的RISC-V。这些多个CPU共享物理RAM，并且xv6利用这种共享来维护所有CPU都可以读写的数据结构。这种共享提出了一个可能性：当一个CPU在另一个CPU正在更新数据结构时读取它，或者甚至多个CPU同时更新相同的数据；如果不经过仔细设计，这种并行访问很可能产生不正确的结果或损坏的数据结构。即使在单处理器上，内核也可能在多个线程之间切换CPU，使它们的执行交错。最后，如果在错误的时间发生中断，可能会损坏与某些可中断代码修改相同数据的设备中断处理程序的数据。并发性是指由于多处理器并行性、线程切换或中断，多个指令流被交错。

内核中充满了并发访问的数据。例如，两个CPU可以同时调用 `kalloc`，从而并发地从空闲列表的头部弹出。内核设计师喜欢允许大量的并发，因为这可以通过并行性提高性能，并提高响应速度。然而，结果是内核设计者必须在并发情况下确保正确性。有许多方法可以得到正确的代码，有些比其他方法更容易进行推理。针对并发正确性的策略，以及支持它们的抽象，被称为并发控制技术。

Xv6根据情况使用多种并发控制技术；还有更多的可能性。本章主要介绍一种广泛使用的技术：锁。锁提供了互斥，确保一次只有一个CPU可以持有锁。如果程序员为每个共享数据项关联一个锁，并且在使用项时始终持有关联的锁，那么一次只有一个CPU可以使用该项。在这种情况下，我们说锁保护了数据项。尽管锁是一种容易理解的并发控制机制，但锁的缺点是它们会限制性能，因为它们会序列化并发操作。

本章的其余部分解释了为什么 xv6 需要锁，如何实现它们，以及如何使用它们。

## 竞争

为了说明为什么我们需要锁，请考虑两个在两个不同CPU上调用 `wait` 的进程，它们都有已退出的子进程。`wait` 会释放子进程的内存。因此，在每个CPU上，内核将调用 `kfree` 释放子进程的内存页面。内核分配器维护一个链表：`kalloc()` 从空闲页面列表中弹出一个内存页面，而 `kfree()` 将一个页面推入空闲列表。为了获得最佳性能，我们可能希望两个父进程的 `kfree` 能够并行执行，而无需等待彼此，但鉴于xv6的 `kfree` 实现，这是不正确的。

![](http://xv6.dgs.zone/tranlate_books/book-riscv-rev1/images/c6/p1.png)

图中详细说明了这个设置：空闲页面的链表位于两个CPU共享的内存中，这些CPU使用加载和存储指令操作链表。（实际上，处理器有缓存，但从概念上讲，多处理器系统的行为就好像有一个单一的共享内存。）如果没有并发请求，您可能会像下面这样实现列表 `push` 操作：

```c
struct element {
  int data;
  struct element *next;
};

struct element *list = 0;

void
push(int data)
{
  struct element *l;
   
  l = malloc(sizeof *l);
  l->data = data;
  l->next = list;
  list = l;
}
```



![](http://xv6.dgs.zone/tranlate_books/book-riscv-rev1/images/c6/p2.png)

如果单独执行，这个实现是正确的。然而，如果有多个副本同时执行，这段代码是不正确的。如果两个CPU同时执行 `push`，它们都可能在执行第 16 行之前执行行 第15行，如图 6.1 所示，从而导致图 6.2 所示的不正确结果。那么会有两个列表元素，其中 `next` 被设置为 `list` 的前一个值。当两个赋值在第 16 行处发生时，第二个赋值将覆盖第一个赋值；第一个赋值涉及的元素将丢失。


第16行的丢失更新是一个竞争条件(race condition)的例子。竞争是指内存位置被并发访问的情况，至少有一个访问是写操作。竞争通常是一个 bug 的迹象，要么是丢失的更新（如果访问是写操作），要么是读取了一个未完全更新的数据结构。竞争的结果取决于编译器生成的机器代码、涉及的两个CPU的时序以及内存系统对它们的内存操作进行排序的方式，这可能导致由于竞争引起的错误难以复现和调试。例如，在调试 `push` 时添加打印语句可能会改变执行的时序，足以使竞争消失。

避免竞争的通常方法是使用锁。锁确保了互斥，这样一次只有一个CPU可以执行 `push` 的敏感行；这使得上面的情况不可能发生。正确锁定版本的上述代码只需添加几行（以黄色突出显示）：

```c
struct element *list = 0;
struct lock listlock;

void
push(int data)
{
  struct element *l;
  l = malloc(sizeof *l);
  l->data = data;
  acquire(&listlock);
  l->next = list;
  list = l;
  release(&listlock);
}
```

`acquire` 和 `release` 之间的指令序列通常称为临界区。锁通常被认为是保护 `list`。

当我们说锁保护数据时，我们实际上是指锁保护了适用于数据的一系列不变量。不变量是跨操作维护的数据结构的属性。通常，操作的正确行为取决于在操作开始时不变量是正确的。操作可能会暂时违反不变量，但必须在完成之前重新建立它们。

例如，在链表的情况下，不变量是`list`指向列表中的第一个元素，以及每个元素的`next`字段指向下一个元素。`push`实现暂时违反了这个不变量：在第 17 行，`l`指向下一个列表元素，但是`list`还没有指向`l`（在第 18 行重新建立）。我们之前检查的竞争发生在第二个 CPU 执行依赖于列表不变量的代码时，而不变量被（暂时）违反了。正确使用锁确保同一时间只有一个 CPU 可以在临界区中操作数据结构，这样就不会有 CPU 在数据结构的不变量不成立时执行数据结构操作。

你可以将锁视为将并发临界区序列化，使它们一次运行一个，从而保留不变量（假设临界区在孤立情况下是正确的）。你还可以将由相同锁保护的临界区视为相对于彼此是原子的，因此每个临界区只能看到来自较早的临界区的完整更改集合，而不会看到部分完成的更新。

尽管锁对于正确性很有用，但它们本质上会限制性能。例如，如果两个进程同时调用`kfree`，锁将对两个临界区进行序列化，因此在不同 CPU 上运行它们没有好处。我们说多个进程在同时需要相同锁的情况下发生**冲突**，或者说锁经历了**竞争**。内核设计中的一项重要挑战是避免锁竞争以追求并行性。Xv6 并没有做很多这方面的工作，但是复杂的内核会特意组织数据结构和算法以避免锁竞争。在列表示例中，内核可能为每个 CPU 维护一个单独的空闲列表，并且仅在当前 CPU 的列表为空且必须从另一个 CPU 窃取内存时才会接触另一个 CPU 的空闲列表。其他用例可能需要更复杂的设计。

锁的放置对于性能也很重要。例如，将 `acquire` 在 `push` 中提前，放在第 13 行之前是正确的。但这可能会降低性能，因为这样对 `malloc` 的调用会被序列化。下面的 "Using locks"（使用锁）部分提供了一些关于在何处插入 `acquire` 和 `release` 调用的指南。

## 代码：锁

Xv6 有两种类型的锁：自旋锁和睡眠锁。我们先从自旋锁开始。Xv6 使用 `struct spinlock` (kernel/spinlock.h:2)表示自旋锁。结构中的重要字段是 `locked`，当锁可用时，该字段值为零；当锁被持有时，该字段值为非零。从逻辑上讲，xv6 应该通过执行如下代码来获取锁：

   ```c
void
   acquire(struct spinlock *lk) // 不起作用！
   {
     for(;;) {
       if(lk->locked == 0) {
         lk->locked = 1;
         break;
       }
     }
   }
   ```

不幸的是，这个实现在多处理器上不能保证互斥。可能出现两个 CPU 同时到达第 25 行的情况，发现 `lk->locked` 为零，然后通过执行第 26 行来获取锁。此时，两个不同的 CPU 持有锁，违反了互斥属性。我们需要一种方法使得第 25 行和第 26 行作为一个原子(*atomic*)（即不可分割）步骤执行。

由于锁被广泛使用，多核处理器通常提供执行 第 25 行和第 26 行的原子版本的指令。在 RISC-V 上，这个指令是 `amoswap r, a`。`amoswap` 读取内存地址 `a` 处的值，将寄存器  `r`  的内容写入该地址，并将其读取到的值放入 `r`。也就是说，它交换寄存器和内存地址的内容。它使用特殊的硬件以原子方式执行这个序列，阻止其他 CPU 在读和写之间使用内存地址。

Xv6 的 `acquire` (kernel/spinlock.c:22) 使用便携式 C 库调用 `__sync_lock_test_and_set`，它归结为 `amoswap` 指令。返回值是 `lk->locked` 的旧（交换后）内容。`acquire` 函数将交换包装在循环中，重试（自旋）直至获取到锁。每次迭代都会将 1 交换到 `lk->locked` 并检查先前的值；如果先前的值为零，那么我们就获取到了锁，交换操作将使 `lk->locked` 的值变为 1。如果先前的值为 1，那么其他 CPU 持有锁，我们原子地将 1 交换到 `lk->locked` 并不会改变其值。

一旦获取到锁，`acquire` 函数会记录获取锁的 CPU，以便于调试。`lk->cpu` 字段受锁保护，只能在持有锁的情况下更改。

函数 `release` (kernel/spinlock.c:47) 和 `acquire` 相反：它清除 `lk->cpu` 字段，然后释放锁。从概念上讲，释放只需将 `lk->locked` 赋值为零即可。C 标准允许编译器使用多个存储指令来实现赋值，因此 C 赋值可能对并发代码来说是非原子的。相反，`release` 使用 C 库函数 `__sync_lock_release` 来执行原子赋值。此函数同样可以归结为 RISC-V 的 `amoswap` 指令。

## 代码：使用锁

Xv6 在许多地方使用锁以避免竞争。如上所述，`kalloc` (kernel/kalloc.c:69) 和 `kfree` (kernel/kalloc.c:47)  是一个很好的例子。尝试练习 1 和 2，看看如果这些函数省略锁会发生什么。你可能会发现触发错误行为是很困难的，这说明要可靠地测试代码是否免受锁定错误和竞争是困难的。Xv6 可能还有尚未发现的竞争。

使用锁的难点之一是决定要使用多少锁，以及每个锁应该保护哪些数据和不变式。有一些基本原则。首先，任何时候一个变量可以被一个 CPU 写入，同时另一个 CPU 可以读取或写入它，都应该使用锁来防止这两个操作重叠。其次，要记住锁保护的是不变量：如果一个不变量涉及多个内存位置，通常需要用单个锁保护所有这些位置，以确保不变量得以维持。

上述规则说明了什么时候需要锁，但没有说明什么时候不需要锁，为了效率，不要使用过多的锁，因为锁会降低并行性。如果并行性不重要，那么可以安排只有一个线程，而不用担心锁。在多处理器上，一个简单的内核可以通过在进入内核时获取一个锁，在退出内核时释放锁来实现这一点（尽管阻塞系统调用，如管道读取或 `wait` 会引起问题）。许多单处理器操作系统已经使用这种方法（有时称为“大内核锁”）转换为在多处理器上运行，但这种方法牺牲了并行性：一次只有一个 CPU 可以在内核中执行。如果内核执行了大量计算，那么使用更大的、更细粒度的锁集合会更有效率，这样内核可以在多个 CPU 上同时执行。

以 xv6 的 `kalloc.c` 分配器为例，它使用了一个受单个锁保护的空闲列表。如果不同 CPU 上的多个进程同时尝试分配页面，那么每个进程都必须通过在 `acquire` 中自旋等待轮到自己。自旋浪费了 CPU 时间，因为它不是有用的工作。如果争用锁浪费了相当一部分 CPU 时间，也许可以通过改变分配器设计来提高性能，使其具有多个空闲链表，每个链表都有自己的锁，以实现真正的并行分配。

作为细粒度锁的一个例子，xv6 为每个文件设置了单独的锁，这样操作不同文件的进程通常可以在不等待彼此锁的情况下进行。如果允许进程同时写入同一文件的不同区域，文件锁方案可以更细粒度。最终，锁粒度的决策需要根据性能测量和复杂性考虑来驱动。

随后的章节将解释 xv6 的每个部分，它们将提及 xv6 使用锁处理并发的示例。作为预览，图 6.3 列出了 xv6 中的所有锁。

| **Lock**         | **Description**                   |
| ---------------- | --------------------------------- |
| bcache.lock      | 保护块缓冲区高速缓存条目的分配    |
| cons.lock        | 串行访问控制台硬件，避免输出混杂  |
| ftable.lock      | 串行化文件表中 struct file 的分配 |
| itable.lock      | 保护内存中的 inode 条目的分配     |
| vdisk_lock       | 串行访问磁盘硬件和 DMA 描述符队列 |
| kmem.lock        | 串行化内存分配                    |
| log.lock         | 串行化对事务日志的操作            |
| pipe's pi->lock  | 串行化对每个管道的操作            |
| pid_lock         | 串行化对 next_pid 的增加          |
| proc's p->lock   | 串行化对进程状态的更改            |
| wait_lock        | 帮助 wait 避免丢失唤醒            |
| tickslock        | 串行化对 ticks 计数器的操作       |
| inode's ip->lock | 串行化对每个 inode 及其内容的操作 |
| buf's b->lock    | 串行化对每个块缓冲区的操作        |

## 死锁和锁顺序

如果内核中的代码路径需要同时持有多个锁，那么所有代码路径按相同顺序获取这些锁非常重要。如果没有这样做，就有死锁的风险。假设 xv6 中有两个代码路径需要锁 A 和锁 B，但代码路径 1 按照 A 到 B 的顺序获取锁，而另一个路径按照 B 到 A 的顺序获取锁。假设线程 T1 执行代码路径 1 并获得锁 A，线程 T2 执行代码路径 2 并获得锁 B。接下来，T1 将尝试获取锁 B，而 T2 将尝试获取锁 A。两个获取操作都会无限期地阻塞，因为在这两种情况下，另一个线程持有所需的锁，并且在其获取操作返回之前都不会释放它。为了避免这样的死锁，所有代码路径都必须按相同的顺序获取锁。全局锁获取顺序的需求意味着锁实际上是每个函数规范的一部分：调用者必须以一种使锁按照约定顺序获取的方式调用函数。

由于 sleep 函数的工作方式（参见第 7 章），xv6 中有许多涉及每个进程锁（在每个 struct proc 中的锁）的长度为 2 的锁顺序链。例如，consoleintr (kernel/console.c:136) 是处理键入字符的中断例程。当换行符到达时，应唤醒等待控制台输入的任何进程。为此，consoleintr 在调用 wakeup 时持有 cons.lock，后者在唤醒进程时获取等待进程的锁。因此，全局避免死锁的锁顺序包括 cons.lock 必须在任何进程锁之前获取的规则。文件系统代码包含了 xv6 最长的锁链。例如，创建一个文件需要同时持有目录上的锁、新文件 inode 上的锁、磁盘块缓冲区上的锁、磁盘驱动程序的 vdisk\_lock 以及调用进程的 p->lock。为了避免死锁，文件系统代码总是按照前面提到的顺序获取锁。

遵守全局避免死锁的顺序可能出奇地困难。有时锁顺序与逻辑程序结构相冲突，例如，可能代码模块 M1 调用模块 M2，但锁顺序要求在 M2 中的锁在 M1 中的锁之前获得。有时锁的身份事先未知，可能是因为必须持有一个锁才能发现下一个要获取的锁的身份。这种情况出现在文件系统查找路径名中的连续组件时，以及在 wait 和 exit 的代码中，它们在进程表中搜索子进程。最后，死锁危险通常是对锁粒度划分的限制，因为更多的锁通常意味着更多的死锁机会。避免死锁通常是内核实现中的一个主要因素。

## 可重入锁

可能会觉得使用可重入锁（也称为递归锁）可以避免一些死锁和锁顺序问题。这里的想法是，如果锁已经被某个进程持有，并且该进程试图再次获取锁，那么内核可以允许这样做（因为进程已经持有了锁），而不是像 xv6 内核那样调用 panic。

然而，事实证明，可重入锁使得推理并发性变得更加困难：可重入锁破坏了关于临界区相对于其他临界区是原子性的锁定直觉。考虑以下两个函数 `f` 和 `g`：

```c
struct spinlock lock;
int data = 0; // 由 lock 保护

f() {
  acquire(&lock);
  if(data == 0){
    call_once();
    h();
    data = 1;
  }
  release(&lock);
}

g() {
  acquire(&lock);
  if(data == 0){
    call_once();
    data = 1;
  }
  release(&lock);
}
```

观察这段代码，直觉是 `call_once` 函数只会被调用一次：要么由 `f` 调用，要么由 `g` 调用，但不会同时被两者调用。

但是，如果允许可重入锁，并且 `h` 恰好调用了 `g`，那么 `call_once` 函数将会被调用两次。

如果不允许可重入锁，那么 `h` 调用 `g` 将导致死锁，这也不是很好。但是，假设调用 `call_once` 函数是一个严重的错误，死锁是更可取的。内核开发者会注意到死锁（内核出现了panic），并可以修复代码以避免死锁，而调用 `call_once` 两次可能会默默地导致一个难以追踪的错误。

出于这个原因，xv6 使用了更容易理解的非可重入锁。只要程序员记住锁定规则，无论哪种方法都可以工作。如果 xv6 使用可重入锁，就必须修改 `acquire` 以注意到锁当前是由调用线程持有的。还必须像 `push_off` 那样向结构体 spinlock 添加嵌套获取的计数。

## 锁与中断处理程序

在 xv6 中，有些自旋锁保护的数据同时被线程和中断处理程序使用。例如，定时器中断处理程序 `clockintr` 可能在与内核线程读取 `ticks` 的同时递增 `ticks` (kernel/trap.c:164)。而内核线程在 `sys_sleep` (kernel/sysproc.c:59)中读取 `ticks` 。锁 `tickslock` 对这两个访问进行序列化。

自旋锁与中断之间的交互引发了一个潜在的危险。假设 `sys_sleep` 持有 `tickslock`，并且其 CPU 被定时器中断中断。`clockintr` 会尝试获取 `tickslock`，发现它被持有，然后等待它被释放。在这种情况下，`tickslock` 永远不会被释放：只有 `sys_sleep` 可以释放它，但是在 `clockintr` 返回之前，`sys_sleep` 不会继续运行。因此，CPU 会发生死锁，任何需要这两个锁的代码也会被冻结。

为了避免这种情况，如果一个自旋锁被中断处理程序使用，CPU 在持有该锁时必须禁用中断。xv6 更加保守：当一个 CPU 获取任何锁时，xv6 总是禁用该 CPU 上的中断。其他 CPU 上的中断仍然可能发生，所以中断的 `acquire` 可以等待线程释放一个自旋锁；只是不能在同一个 CPU 上。

当 CPU 不持有任何自旋锁时，xv6 会重新启用中断；它必须进行一些记录来应对嵌套的临界区。`acquire` 调用 `push_off` (kernel/spinlock.c:89)`release` 调用 `pop_off` (kernel/spinlock.c:100)，以跟踪当前 CPU 上锁的嵌套级别。当计数达到零时，`pop_off` 恢复最外层临界区开始时存在的中断启用状态。`intr_off` 和 `intr_on` 函数分别执行 RISC-V 指令以禁用和启用中断。

重要的是，`acquire` 在设置 `lk->locked` (kernel/spinlock.c:28)之前严格调用 `push_off` 。如果两者颠倒，当锁被持有且中断被启用时，会有一个短暂的窗口，不幸的是，定时中断会使系统死锁。同样，重要的是，`release` 只有在释放锁之后才调用 `pop_off` (kernel/spinlock.c:66)。

## 指令与内存排序

在单线程代码中，根据源代码语句出现的顺序来思考程序执行是很自然的。但是，在多线程通过共享内存进行交互时，这种想法是不正确的。原因之一是编译器按照与源代码隐含的顺序不同的顺序发出加载和存储指令，甚至可能完全省略它们（例如通过将数据缓存在寄存器中）。另一个原因是 CPU 可能会为了提高性能而对指令进行乱序执行。例如，CPU 可能会注意到在一个串行序列的指令 A 和 B 中，它们彼此不依赖。CPU 可能首先启动指令 B，要么因为它的输入在 A 的输入之前就绪，要么为了使 A 和 B 的执行重叠。

以 `push` 的代码为例，如果编译器或 CPU 将与第 4 行对应的存储移动到第 6 行上的 `release` 之后，这将是一个灾难：

```c
l = malloc(sizeof *l);
l->data = data;
acquire(&listlock);
l->next = list;
list = l;      
release(&listlock);
```

如果发生这样的重新排序，那么在其他 CPU 可以获取锁并观察到更新后的 `list` 的同时，将会出现一个窗口，但看到未初始化的 `list->next`。

好消息是编译器和 CPU 通过遵循一组称为内存模型(*memory model*)的规则以及提供一些原语来帮助程序员控制重新排序，从而帮助并发程序员。

为了告诉硬件和编译器不要重新排序，xv6 在 `acquire`  (kernel/spinlock.c:22) 和 `release` (kernel/spinlock.c:47) 中都使用了 `__sync_synchronize()`。`__sync_synchronize()` 是一个内存屏障(*memory barrier*)：它告诉编译器和 CPU 不要在屏障两侧重新排序加载或存储。xv6 的 `acquire` 和 `release` 中的屏障在几乎所有需要排序的情况下都强制排序，因为 xv6 在访问共享数据时使用了锁。第 9 章讨论了一些例外。


## 睡眠锁

有时 xv6 需要长时间持有一个锁。例如，文件系统（第 8 章）在将文件内容读取和写入磁盘时保持文件锁定，这些磁盘操作可能需要数十毫秒。长时间持有一个自旋锁会导致浪费，如果其他进程想要获取它，因为获取进程在自旋时会浪费很长时间的 CPU。自旋锁的另一个缺点是进程在保留自旋锁的同时无法放弃 CPU；我们希望这样做是为了让其他进程在持有锁的进程等待磁盘时使用 CPU。在持有自旋锁的情况下放弃 CPU 是非法的，因为如果第二个线程试图获取自旋锁，可能会导致死锁；由于 `acquire` 不会放弃 CPU，第二个线程的自旋可能会阻止第一个线程运行并释放锁。在持有锁的情况下放弃锁还会违反在持有自旋锁时中断必须关闭的要求。因此，我们希望有一种锁，在等待获取时放弃 CPU，同时在锁持有时允许放弃（和中断）。

Xv6 以 睡眠锁(*sleep-locks*) 的形式提供了这样的锁。`acquiresleep`  (kernel/sleeplock.c:22) 在等待时放弃 CPU，使用第 7 章将要介绍的技术。从高层次来看，睡眠锁有一个受自旋锁保护的 `locked` 字段，`acquiresleep` 对 `sleep` 的调用原子地放弃 CPU 并释放自旋锁。结果是其他线程可以在 `acquiresleep` 等待时执行。

因为睡眠锁保持中断开启，所以它们不能在中断处理程序中使用。由于 `acquiresleep` 可能会放弃 CPU，所以在自旋锁临界区中不能使用睡眠锁（尽管在睡眠锁临界区内可以使用自旋锁）。

自旋锁最适合短临界区，因为等待它们会浪费 CPU 时间；对于长时间的操作，睡眠锁效果很好。


## 真实世界

尽管对并发原语和并行性进行了多年的研究，使用锁编程仍然具有挑战性。最好是将锁隐藏在像同步队列这样的高级构造中，尽管 xv6 并没有这样做。如果使用锁进行编程，最好使用一个试图识别竞争的工具，因为很容易忽略需要锁的不变式。

大多数操作系统支持 POSIX 线程（Pthreads），允许用户进程在不同的 CPU 上并发运行多个线程。Pthreads 支持用户级锁、屏障等。Pthreads 还允许程序员选择性地指定可重入锁。

在用户级别支持 Pthreads 需要操作系统的支持。例如，如果一个 pthread 在系统调用中阻塞，同一进程的另一个 pthread 应该能够在该 CPU 上运行。另一个例子是，如果一个 pthread 更改其进程的地址空间（例如，映射或取消映射内存），内核必须安排运行同一进程的线程的其他 CPU 更新其硬件页面表以反映地址空间的更改。

可以在没有原子指令的情况下实现锁，但代价昂贵，大多数操作系统使用原子指令。

如果许多 CPU 同时尝试获取相同的锁，锁可能会很昂贵。如果一个 CPU 将锁缓存在其本地缓存中，并且另一个 CPU 必须获取锁，那么更新持有锁的缓存行的原子指令必须将该行从一个 CPU 的缓存移动到另一个 CPU 的缓存，并可能使任何其他缓存行的副本失效。从另一个 CPU 的缓存中获取缓存行比从本地缓存中获取缓存行要贵几个数量级。

为了避免与锁相关的开销，许多操作系统使用无锁数据结构和算法。例如，可以实现一个像本章开头的链表，该链表在链表搜索期间不需要锁，而在链表中插入一个项目只需要一个原子指令。然而，无锁编程比编程锁更复杂；例如，必须担心指令和内存重新排序。使用锁编程已经很难，所以 xv6 避免了无锁编程的额外复杂性。